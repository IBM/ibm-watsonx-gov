"input_text","ground_truth"
"To run Watson OpenScale on my own servers, how much computer processing power is required?","There are specific guidelines for hardware configuration for three-node and six-node configurations. Your IBM Technical Sales team can also help you with sizing your specific configuration."
"Why does Watson OpenScale need access to training data?","Watson OpenScale needs access to training data to generate explanations, display training statistics, and build a drift detection model."
"Is there a command-line tool to use with Watson OpenScale?","Yes, there is a ModelOps CLI tool called Watson OpenScale CLI model operations tool."
"What version of Python can I use with Watson OpenScale?","Watson OpenScale is independent of your model-creation process and supports any Python version that your machine learning provider supports."
"Which browsers can I use to run Watson OpenScale?","Watson OpenScale service tooling requires the same level of browser software as required by IBM Cloud."
"Can Watson OpenScale use Netezza for training data?","Yes, use a Watson OpenScale Notebook to read the data from Netezza and generate training statistics and a drift detection model."
"Why doesn't Watson OpenScale see the updates that were made to the model?","Watson OpenScale works on a deployment of a model, not the model itself. A new deployment must be created and configured."
"How is model bias mitigated in Watson OpenScale?","Watson OpenScale uses a two-step process: Learning Phase (understanding bias in the model) and Application Phase (fixing the bias)."
"Can Watson OpenScale check for model bias on sensitive attributes like race and sex?","Yes, Watson OpenScale offers Indirect Bias detection to detect bias even when the model is not trained on sensitive attributes."
"Can I configure model fairness through an API?","Yes, it is possible with the Watson OpenScale SDK."
"What are the risks associated with using a machine learning model?","Model risks include data drift, runtime bias, and lack of interpretability, leading to inaccurate decisions."
"Does Watson OpenScale detect drift in accuracy and data?","Yes, Watson OpenScale detects both accuracy drift and data drift."
"What types of explanations does Watson OpenScale provide?","Watson OpenScale provides Local explanation (LIME), contrastive explanation, and SHAP explanations."
"What is what-if analysis in Watson OpenScale?","What-if analysis allows users to modify feature values and see the impact on model predictions."
"Which models support LIME explanations?","LIME explanations are supported for structured regression and classification models, as well as unstructured text and image classification models."
"Which models support contrastive explanations and what-if analysis?","Contrastive explanations and what-if analyses are supported for structured data classification models."
"What are controllable features in Watson OpenScale explainability configuration?","Controllable features can be locked to prevent changes in contrastive explanations and what-if analysis."
"Must I keep monitoring the Watson OpenScale dashboard to ensure model behavior?","No, email alerts can be set up for production model deployments to notify users when a risk evaluation test fails."
"Do I need to reconfigure monitors in production after evaluating a model in pre-production?","No, Watson OpenScale allows copying pre-production configurations to the production subscription."
"Can I compare my model deployments in pre-production with a benchmark model?","Yes, Watson OpenScale allows comparison of two model deployments or subscriptions side by side."
"What data is used for quality metrics computation in Watson OpenScale?","Quality metrics use manually labeled feedback data and monitored deployment responses."
"Why are some configuration tabs disabled in Watson OpenScale?","Certain conditions enable particular tabs. Hovering over the circle icon on the tab explains why it is disabled."
"Why is model accuracy drift or data drift concerning?","A drop in model accuracy or data consistency negatively impacts business outcomes and requires model retraining."
"How does Watson OpenScale detect model accuracy drift?","Watson OpenScale creates a proxy model (drift detection model) to estimate accuracy drift based on training data."
"How does Watson OpenScale detect data consistency drift?","It learns single and two-column constraints on training data and evaluates runtime transactions against these constraints."
"Can Watson OpenScale detect drift in text-based models?","No, Watson OpenScale currently does not support drift detection for text-based models."
"Can Watson OpenScale detect drift in image-based models?","No, Watson OpenScale currently does not support drift detection for image-based models."
"Can Watson OpenScale detect drift in Python functions deployed on IBM watsonx.ai Runtime?","Yes, for Python functions trained on structured datasets with proper prediction and probability columns."
"How is 'acceptable fairness' calculated?","By multiplying perfect equality by the fairness threshold."  
"What triggers an 'alert' in model monitoring?","When a performance metric falls outside the acceptable range set by monitors."  
"Can you give an example of a 'balanced data set'?","A data set containing scoring requests and perturbed records for a selected hour."  
"Why is 'baseline data' important?","It serves as a reference point for comparing future data after interventions."  
"What is the difference between 'batch deployment' and online deployment?","Batch deployment processes data from files/storage, while online deployment provides real-time scoring via an API."  
"When should 'batch processing' be used?","When monitoring deployments with large payload or feedback data."  
"How does 'bias' manifest in machine learning models?","When a model produces unfair results for a monitored group compared to a reference group."  
"What credentials are needed to access IBM's 'Cloud Object Storage'?","The associated service credentials linked to the machine learning assets."  
"How is a 'confidence score' interpreted?","A higher score means the model's prediction is more likely to match the actual outcome."  
"What does a 'contrastive explanation' reveal?","The minimal feature changes needed to alter a model's prediction for a single data point."  
"What is stored in a 'data mart'?","Metadata from model evaluations, connected to a backend database."  
"How are 'debiased transactions' generated?","By adjusting outcomes to mitigate detected bias in monitored groups."  
"What actions follow 'debiasing'?","Steps to automatically or manually correct biased outcomes."  
"What are the two types of model 'deployment' environments?","Pre-production (testing) and production (actual usage)."  
"What causes 'drift' in model performance?","Changes in input data leading to declining accuracy over time."  
"What metrics are used in model 'evaluation'?","Fairness, accuracy, and other performance indicators relevant to goals."  
"How do 'explanations' aid model debugging?","By providing insights into evaluations and enabling what-if scenario testing."  
"What categories are typically monitored for 'fairness'?","Age, sex, and race."  
"Name two 'features' in a loan approval model.","Employment status and credit history."  
"Why is 'feedback data' used in the Quality monitor?","To compare model predictions against known outcomes for accuracy checks."  
"What distinguishes a 'global explanation'?","It explains predictions across a sample of data, not just single instances."  
"How does a 'headless subscription' work?","It monitors deployments using payload/feedback data without a scoring URL."  
"Give an example of 'labeled data'.","A table with labeled columns for supervised learning, or tagged images."  
"What is the focus of a 'local explanation'?","A model's prediction for a specific, individual example."  
"How are 'meta-fields' used?","To handle specialized data unique to different products."  
"Name three types of model 'monitors'.","Fairness, drift, and quality monitors."  
"Who defines the 'monitored group' in fairness checks?","The user, based on groups at risk for biased outcomes (e.g., 'Female' for sex)."  
"What is the advantage of an 'online deployment'?","Real-time scoring via API for immediate results."  
"What components make up 'payload data'?","Model input (requests) and output (responses)."  
"Why is 'payload logging' useful?","To persist and analyze real-time model interactions."  
"How is 'perfect equality' measured?","By the percentage of favorable outcomes given to all reference groups."  
"What is tested in a 'pre-production space'?","Model validations before live deployment."  
"What does the 'prediction column' represent?","The target variable predicted by a supervised model."  
"How is 'probability' used in classification models?","As a confidence measure for predicted outputs."  
"What is the purpose of a 'production space'?","To operationalize models and evaluate real-world performance."  
"What does the 'Quality' monitor assess?","Prediction accuracy against labeled feedback data."  
"What are 'records' in monitoring?","Transactions evaluated by monitors."  
"How is a 'reference group' selected?","As the cohort least at risk for bias (e.g., ages 30-55)."  
"What does 'relative weight' indicate?","A feature's importance in predicting the target variable."  
"How do you obtain a 'Resource ID' in IBM Cloud?","By expanding a resource in the Cloud dashboard and copying the ID."  
"What affects 'response time' in deployments?","The processing speed for scoring requests."  
"What is included in 'runtime data'?","Data from a model's lifecycle execution."  
"How is a 'scoring endpoint' accessed?","Via an HTTPS API call to a deployed model."  
"What is a 'scoring request'?","Input data sent to a model for prediction."  
"What does 'scoring' involve?","Sending data to a model and receiving its output."  
"What distinguishes 'self-managed' transactions?","Storage in a user's data warehouse and evaluation via their Spark engine."  
"What are 'service credentials' used for?","Accessing IBM Cloud resources like storage."  
"Who is a 'Service Provider' in ML?","A model engine (e.g., WML, AWS, Azure) hosting deployments."  
"What is the relationship between 'subscription' and deployment?","A 1-to-1 mapping where each deployment is monitored as a subscription."  
"How does 'system-managed' evaluation work?","Transactions are stored and evaluated using IBM's database and resources."  
"What is the 'target' in model training?","The feature/column the model is trained to predict."  
"What happens when a 'threshold' is breached?","An alert triggers to address the issue."  
"What role does 'training data' play?","It teaches the model's algorithm patterns and relationships."   
"Give an example of 'unlabeled data'.","Emails or raw images without uniform tags."  
"How is a 'User ID' used?","To associate scoring requests with specific users."  
"What is the formula for 'acceptable fairness'?","Perfect equality * fairness threshold."  
"Can alerts be customized?","Yes, based on metric ranges defined in monitors."  
"How are perturbed records created?","By simulating data variations around real points."  
"Why compare against baseline data?","To measure the impact of changes over time."  
"Does batch deployment support real-time scoring?","No, it processes data in bulk from files."  
"When is batch processing preferred?","For large-scale data to avoid real-time overhead."  
"Can bias occur in training data?","Yes, skewed data may lead to unfair model outputs."  
"Is Cloud Object Storage mandatory for IBM models?","No, but it requires credentials if used."  
"Does confidence score apply to regression models?","No, it's primarily for classification."  
"Are contrastive explanations model-agnostic?","Yes, they work across models for single instances."  
"Can data marts be shared across teams?","Yes, if they have access to the workspace."  
"Is debiasing always automatic?","No, it can require manual intervention."  
"Can deployments be updated without downtime?","Yes, depending on the platform's capabilities."  
"Is drift always negative?","Yes, it signals declining model performance."  
"Are evaluations only for production models?","No, they're used in pre-production too."  