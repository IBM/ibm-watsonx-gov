{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Metrics\n",
    "This notebook evaluates RAG metrics using IBM watsonx.governance SDK. It can evaluate RAG metrics by taking in the data containing contexts, question, answer and ground truth (Optional) information. The metrics result will be visualized using the `ModelInsights`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should be run in Python 3.10 or greater runtime environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "- Compute RAG metrics using `evaluate_metrics`.\n",
    "- Visualize metrics result using `ModelInsights`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Step 1 - Install libraries](#install)\n",
    "- [Step 2 - Configuration](#configuration)\n",
    "- [Step 3 - Evaluate Metrics](#evaluate)\n",
    "- [Step 4 - Display the results](#display)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries<a name=\"install\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"ibm-watsonx-gov[metrics,visualization]\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you may need to restart the kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration <a name=\"configuration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your watsonx.governance credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.config import Credentials\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=\"<EDIT_THIS>\",\n",
    "    api_key=\"<EDIT_THIS>\",\n",
    "    service_instance_id=\"<EDIT_THIS>\",\n",
    "\n",
    "    # Uncomment the following attributes when using watsonx.governance\n",
    "    # username=\"<EDIT_THIS>\",\n",
    "    # version=\"<EDIT_THIS>\",\n",
    "    # disable_ssl=\"<EDIT_THIS>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Metrics<a name=\"evaluate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the input to the RAG application, its generated output, and the retrieved contexts are loaded. To get started, please use the sample CSV dataset file provided in this notebook. However, if you want to use your own RAG application and would like to evaluate it's output, the dataset can be loaded in the following cell and the configuration can be updated to read the specific columns in the dataset.\n",
    "\n",
    "### Loading your test data\n",
    "\n",
    "In this step, you can load your own sample RAG dataset in `input_df` and update the configuration object in the next step to match the column names from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_df = pd.read_csv(\"https://raw.githubusercontent.com/IBM/ibm-watsonx-gov/refs/heads/samples/notebooks/data/rag/rag_with_ground_truth.csv\")\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the evaluator\n",
    "\n",
    "Once the dataset is loaded, need to configure the evaluator to specify what are the columns of interests in the data set and to specify which metrics to evaluate\n",
    "\n",
    "#### Dataset columns\n",
    "\n",
    "To configure the evaluator for the dataset, please update `question_field` to be the column name that contains the questions, `context_field` to be a list of column names that contais the contexts, and `output_fields` to have the column that contain the generated answer. Optionally, you can add the ground truth column name under `reference_fields`.\n",
    "\n",
    "#### Metrics\n",
    "\n",
    "You can select the metrics to evaluate and using which method under `metrics` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.config import GenAIConfiguration\n",
    "from ibm_watsonx_gov.metrics import ContextRelevanceMetric, FaithfulnessMetric, AnswerSimilarityMetric\n",
    "from ibm_watsonx_gov.entities.enums import TaskType\n",
    "\n",
    "question_field = \"question\"\n",
    "context_field = \"contexts\"\n",
    "\n",
    "config = GenAIConfiguration(\n",
    "    input_fields=[question_field, context_field],\n",
    "    question_field=question_field,\n",
    "    context_fields=[context_field],\n",
    "    output_fields=[\"answer\"],\n",
    "    reference_fields=[\"ground_truth\"],\n",
    "    task_type=TaskType.RAG,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    ContextRelevanceMetric(method=\"sentence_bert_mini_lm\"),\n",
    "    FaithfulnessMetric(method=\"token_k_precision\"),\n",
    "    FaithfulnessMetric(method=\"sentence_bert_mini_lm\"),\n",
    "    AnswerSimilarityMetric(method=\"token_recall\"),\n",
    "    AnswerSimilarityMetric(method=\"sentence_bert_mini_lm\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the metrics evaluation\n",
    "\n",
    "Pass the credentials, configuration, metrics, and the RAG dataset to `evaluate_metrics()` to start the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.evaluate import evaluate_metrics\n",
    "\n",
    "evaluation_result = evaluate_metrics(\n",
    "    credentials=credentials,\n",
    "    configuration=config,\n",
    "    metrics=metrics,\n",
    "    data=input_df,\n",
    "    output_format=\"dataframe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the results  <a name=\"display\"></a>\n",
    "\n",
    "### Display the result table\n",
    "\n",
    "Now that the evaluation is done, display a table containing each record with its metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.visualizations import display_table\n",
    "display_table(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ModelInsights` visualization\n",
    "\n",
    "Finally, the evaluation result is displayed interactively using `ModelInsights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "from ibm_watsonx_gov.visualizations import ModelInsights\n",
    "\n",
    "model_insights = ModelInsights(configuration=config, metrics=metrics)\n",
    "model_insights.display_metrics(metrics_result=evaluation_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrav2test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
